---
title: "Principal Components Analysis"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    incremental: true
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
```


# Overview

- Supervised vs. Unsupervised Learning
- Warm-up: Matrix multiplication
- Principal Components
- PCA in (base) R

For further reading, see [JWHT], pp. 496-510

# Supervised vs. Unsupervised Learning

## Too many variables?

- So far, all of our models  have the form $Y = f(X) + \epsilon$, where $Y$ is a variable we would like to predict, and $X$ represents the predictor variables.
- We "learn" and estimate $\hat{f}$ from a data set.
    - $p =$ the number of predictor variables.
    - $n = $ the number of cases (observations) in our data set.
- **Problem:** What happens if $p$ is close to $n$, or if $p > n$.
    - If $p > n$, the linear regression fit is undefined.

## Supervised vs. Unsupervised Learning

- So far, we have been doing **supervised** learning: 
    - We are trying to learn a prediction function $\hat{f}$.
    - We can test our model quality by using testing values of $(x,y)$.
- In **unsupervised** learning, you only consider the predictor variables $X$, and not $Y$ (or there is no $Y$ to consider.) 
    - Instead of trying to find a prediction function, you are just looking for patterns in the $X$s.
- Two types of unsupervised learning
    - Clustering
    - Dimension reduction (e.g., PCA)

# Warm-up: Matrix multiplication

## Warm-up problem 1: Multiply

$$
\begin{bmatrix}
3 & 1 \\
4 & 2
\end{bmatrix}
\begin{bmatrix}
10 & 1 & 2 \\
5  & 3 & 10   
\end{bmatrix}
$$


## Warm-up problem 2: Covariance

Find a formula for $\text{Cov}(X, Y)$ when $E(X) = 0 = E(Y)$.

# Principal Components

## Principal Components Analysis, informally

- Our data consists of tuples $(x_1, x_2, \ldots, x_p)$.
    - Geometrically, these are points in $p$-dimensional space.
    - WLOG, we can assume each variable $X_i$ has $E(X_i) = 0$.
        - "mean-center" the data.
        - The center of the "data cloud" is at the origin.
    - Can we find a linear subspace (e.g., line, plane, hyperplane) that is "close" to our data cloud.
        - This subspace can have dimension $s < p$.
        - We can project the data cloud onto this subspace.
        
## {data-background-image="images/pca4.png" data-background-size="contain"}

## PCA: Theory 

- Let $\mathbf{X} = \left[X_1 \; X_2 \; \cdots X_p\right]$ be the **data matrix**.
- Since the data is mean-centered, $\text{Cov}(X_i, X_j) = E(X_iX_j)$.
- The pairwise **covariance matrix**, up to a constant, is $\mathbf{X}^T\mathbf{X}$.
    - Since $\mathbf{X}^T\mathbf{X}$ is symmetric, its eigenvectors are *orthogonal*.
    - The *eigenvalues* of $\mathbf{X}^T\mathbf{X}$ are proportional to the variance of the cloud along the corresponding eigenvector.
- **Upshot:** PCA gives us a new coordinate system, with fewer dimensions, such that the variance of the data cloud along the axes is maximized. Thus the data cloud is "close" to this linear subspace.
- If interested, [more details here](https://arxiv.org/pdf/1404.1100.pdf).
    
## {data-background-image="images/pca1.png" data-background-size="contain"}

## {data-background-image="images/pca2.png" data-background-size="contain"}

## PCA: How to use

Given an $n \times p$ data matrix $\mathbf{X}$, PCA returns **loadings** and **scores**. We have $n$ observations of $p$ variables: $i = 1, 2, \ldots, n$, $j = 1, 2, \ldots p$.

- The **loadings** of the $j$th principal component are $\phi_{1j}, \phi_{2j}, \ldots, \phi_{pj}$. 
    - Fact: this is the $j$th eigenvector, as a unit vector.
    - These form the *columns* of a $p \times p$ **rotation matrix** $\Phi$.
        - If you want $k < p$ dimensions, just use the first $k$ columns $\Phi_{(k)}$.
        - $\mathbf{X}\Phi_{(k)}$ gives the projected values onto the $k$-dimensional subspace.
- The $i$th PCA **score** of the $j$th component is $z_{ij} = \phi_{1j}x_{i1} + \phi_{2j}x_{i2} + \cdots + \phi_{pj}x_{ip}$.
    - These form an $n \times p$ matrix $Z = \mathbf{X}\Phi$, whose rows are the principal component scores for each observation.
- Crime example: $p = 4$ (Rape, Assault, Murder, UrbanPop), $k = 2$

## {data-background-image="images/pca3.png" data-background-size="contain"}

## Proportion of Variance Explained (PVE)

- TSS is the total sum of squares of $\mathbf{X}$ (note: mean centered)
- RSS is the residual sum of squares for the $k$-dimensional projection.
- So the **proportion of variance explained** by the first $k$ principal components is:
$$
\text{PVE} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
- A **scree plot** shows PVE versus $k$ (number of components used).

## {data-background-image="images/pca5.png" data-background-size="contain"}

## Another Example: Umpire Characteristics

[New metrics for evaluating home plate umpire consistency and accuracy
](https://doi.org/10.1515/jqas-2018-0061)

See Sections 4.1-4.2: Figure 13, Table 3 and Figure 15. 

# PCA in (base) R