---
title: "Extending the Linear Model"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
```


# Overview

- Interactions
- Nonlinear Terms
- Some experiments in R
- Potential Problems
- $K$-Nearest Neighbors for continuous response variables

For further reading, see [JWHT], pp. 87-121.

## Last time: The Linear Regression Model

In **Linear Regression**, we model $f(X)$ with:

$$
Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
$$

. . .

Easy to predict using $f$, but also to **infer**. 

- Confidence intervals and p-values for the $\beta_i$s.
    - Interpreting multiple variables; conditioning.
- Model fit using $R^2$.
- $F$-test for omnibus association


# Interactions and Nonlinear Terms

## Assumptions of the Linear Model

The model $Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \cdots + \beta_p X_p + \epsilon$ requires some assumptions.

>- **Additivity:** The effect of changing $X_i$ is completely determined by its coefficient $\beta_i$, and has no effect on the other coefficients $\beta_j$, $j\neq i$.
>- **Linearity:** The effect of changing $X_i$ by one unit is always the same, regardless of the value of $X_i$.

## Relaxing the additive assumption

>- Consider the model `sales ~ TV + radio`.
>    - What if increasing `radio` actually makes `TV` advertising more effective?
>    - The `TV` slope should be greater for higher values of `radio`.
>- Replace the model $Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \epsilon$ with $Y = \beta_0 + \beta_1 X_1 +\tilde{\beta}_2 X_2 + \epsilon$, where $\tilde{\beta}_2 = \beta_1 + \beta_3X_2$.
>- This is equivalent to adding an **interaction term** $\beta_3X_1X_2$:
$$
Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \beta_3 X_1X_2 + \epsilon
$$

## Coding Interactions in R

```{r message=FALSE}
library(tidyverse)
Advertising <- read_csv("https://www.statlearning.com/s/Advertising.csv")

## Original model:
adMod1 <- lm(sales ~ TV + radio, data = Advertising)
## Model with interaction term:
adMod2 <- lm(sales ~ TV + radio + TV:radio, data = Advertising)
## Equivalent syntax using *:
adMod2a <- lm(sales ~ TV * radio, data = Advertising)
```

## Original Model

```{r}
summary(adMod1)
```

## Model with TV:radio interaction

```{r}
summary(adMod2) ## adMod2a is exactly the same
```

## The Heirarchical Principle

- The coefficients for `TV` and `radio` model the **main effects**.
- The coefficient for `TV:radio` models the **interaction**.
-  **Hierarchical principle:** If we include an interaction in a model, we should also *include the main effects*, even if the p-values associated with their coefficients are not significant.
    - Reason: the interaction term models a variable slope (see above).

## Example: Quantitative:Categorical interaction

RStudio carpentry exercise.

```{r eval=FALSE, include=FALSE}
library(tidyverse)
library(ISLR2)
glimpse(Credit)
## predict Balance from Income
credMod1 <- lm(Balance ~ Income, data = Credit)
summary(credMod1)
## predict Balance from Income and Student
credMod2 <- lm(Balance ~ Income + Student, data = Credit)
summary(credMod2)
## predict Balance from Income and Student, with an interaction term.
credMod3 <- lm(Balance ~ Income * Student, data = Credit)
summary(credMod3)
credMod4 <- lm(Balance ~ Income, data = filter(Credit, Student == "Yes"))
summary(credMod4)
Credit %>%
  filter(Student == "Yes") %>%
  lm(Balance ~ Income, data = .) ->
  credMod4
summary(credMod4)
Credit %>%
  filter(Student == "No") %>%
  lm(Balance ~ Income, data = .) ->
  credMod5
summary(credMod5)
Credit %>%
  ggplot(aes(x = Income, y = Balance, color = Student)) +
  geom_smooth(method = lm) +
  geom_point()
## Note: interaction term is not significant, so this 
# isn't a great example.


library(tidyverse)
library(palmerpenguins)
glimpse(penguins)
#pen_complete <- penguins %>%
  #select(body_mass_g, flipper_length_mm, species) %>%
#  drop_na()
#alternatively, to demonstrate filter
pen_complete <- penguins %>%
  filter(!is.na(body_mass_g))
ggplot(pen_complete, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
# Challenge: Investigate the penguins example
#predict body mass from flipper length and species
#try with and without interactions
#try body mass from flipper length for each species separately
penMod1 <- lm(body_mass_g ~ flipper_length_mm + species, data = pen_complete)
summary(penMod1)
penMod2 <- lm(body_mass_g ~ flipper_length_mm * species, data = pen_complete)
summary(penMod2)
pen_complete %>%
  filter(species == "Adelie") %>%
  lm(body_mass_g ~ flipper_length_mm, data = .) ->
  penMod3
summary(penMod3)
```


## Dealing with nonlinearity

```{r, fig.align='center', fig.width=6, fig.height=4}
library(ISLR2)
ggplot(Auto, aes(x = mpg, y = horsepower)) + geom_point()
```

## Polynomial Regression

- If the association appears non-linear, you can add higher-power terms.
- Linear model: $Y = \beta_0 + \beta_1 X + \epsilon$
- Quadratic model: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$

```{r}
carLin <- lm(mpg ~ horsepower, data = Auto)
carQuad <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
```

## Linear model summary

```{r}
summary(carLin)
```

## Quadratic model summary

```{r}
summary(carQuad)
```

## Plot the models

```{r, fig.align='center', fig.width=6, fig.height=4}
ggplot(Auto, aes(x = horsepower, y = mpg)) +
  geom_point() +
  geom_smooth(method = lm, formula = y ~ x, color = "green") +
  geom_smooth(method = lm, formula = y ~ x + I(x^2), color = "purple")
```

# Potential Problems

## Model Assumptions

$$
Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
$$

- $\epsilon$ is a normal random variable with $E(\epsilon) = 0$, $\text{Var}(\epsilon) = \sigma^2$ (a constant).
- The $\epsilon$ term is **independent** of the $X_i$'s.
- **Homoscedasticity:** The variability residuals should not change for different values of $X$.
- Likewise, the residuals should not change for different fitted values of $Y$. (plot residuals vs. fit)

## Residuals vs. Fit: Patterns are bad

```{r, fig.align='center', fig.width=7, fig.height=4.5}
plot(carLin, which = 1)
```

## Residuals vs. Fit: Heteroscedasticity is bad

```{r, fig.align='center', fig.width=7, fig.height=4.5}
plot(carQuad, which = 1)
```

## Residuals should be normally distributed

```{r, fig.align='center', fig.width=7, fig.height=4.5}
hist(carLin$residuals)
```


## Normal iff Normal-QQ plot is a straight line

```{r, fig.align='center', fig.width=7, fig.height=4.5}
plot(carLin, which = 2)
```



## {data-background-image="images/hsced.png" data-background-size="contain"}

## {data-background-image="images/tsresids.png" data-background-size="contain"}

## Outliers

- Points that fall well off the regression line are (bivariate) outliers.
- You can see these in the residual plots.
- Rule of thumb: standardized residuales $>3$ are outliers.

## {data-background-image="images/lev1.png" data-background-size="contain"}

## Leverage

- Points at the extremes of the $X$ range are more **influential** on the regression coefficients.
- This extremity can be quantified by the **leverage statistic:**
$$
h_i  = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^n (x_j - \bar{x})^2}
$$
- There's a generalization of this formula for multiple predictors.



## {data-background-image="images/lev2.png" data-background-size="contain"}

## Correlated predictors: Collinearity

- If some of the predictors are correlated, the are called **collinear**.
- This is a problem because it can be hard to discern which predictor is having the effect.
    - The minimum RSS may be (close to) non-unique.
- Credit example
   - Predict balance from age and limit
   - Predict balance from rating and limit
   
## {data-background-image="images/collin1.png" data-background-size="contain"}


## {data-background-image="images/collin2.png" data-background-size="contain"}

# KNN for continuous response variables 

## K-Nearest Neighbors

Recall: You can predict the *probability* that a *categorical* response $Y$ will be $j$ based on a value $x_0$ of the explanatory variable:
$$
\text{Pr}(Y = j \mid X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)
$$

- The sum is taken over a "neighborhood" of nearby points.

. . .

Similarly, you can predict the *value* of a *continuous* response $Y$:

$$
\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i
$$

- The predicted $y$-value at $x_0$ is simply the average of the $K$-nearest $y$-values.

## Flexibility

- Recall, *larger* values of $K$ make KNN *less* flexible.
- Generally, KNN is more flexible than linear regression.

## {data-background-image="images/knnreg1.png" data-background-size="contain"}


## {data-background-image="images/knnreg2.png" data-background-size="contain"}


## {data-background-image="images/knnreg3.png" data-background-size="contain"}


## {data-background-image="images/knnreg4.png" data-background-size="contain"}




