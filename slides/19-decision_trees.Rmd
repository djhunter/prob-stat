---
title: "Decision Trees"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    incremental: false
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
```


# Overview

- Motivation: Finite codomain
- Decision trees for regression
    - Pruning
- Decision trees for classification
- Decision trees using `rpart`.

For further reading, see [JWHT], pp. 327-340

# Motivation: Regression with finitely-many outputs

## Recall: Regression models

- We have numeric predictor variables $X = (X_1, X_2, \ldots, X_p)$.
- We have a numeric response variable $Y$.
- General model: $Y = f(X) + \epsilon$.
- Goal: Given $n$ rows of training data, "learn" a function $\hat{f}$ such that $\text{MSE} = \frac{1}{n}\sum_{i=1}^n (\hat{y}_i - y_i)^2$ is minimized.

. . .

**Special Case:** Consider functions $f$ with *finite codomain*. That is, the predictor function $f$ can only take finitely-many different values.

## Exercise: One possible value

Suppose we have a single predictor variable $X$ and the following data.

$$
\begin{array}{c||c|c|c|c|c}
x & 0 & 2 & 1 & 2 & 3  \\ \hline
y & 10 & 30 & 20 & 35 & 45   
\end{array}
$$

Find a predictor function of the form $\hat{f}(x) = A$, where $A$ is constant, such that the $\text{MSE}$ on the training set is minimized. Justify your answer.

## Two possible values?

$$
\begin{array}{c||c|c|c|c|c}
x & 0 & 2 & 1 & 2 & 3  \\ \hline
y & 10 & 30 & 20 & 35 & 45   
\end{array}
$$

Find a predictor function of the form
$$
\hat{f}(x) = \begin{cases}
A & \text{if } x<k \\
B & \text{if } x\geq k 
\end{cases}
$$
where $A, B, k$ are constant. Your choice should minimize the $\text{MSE}$.

## Recursive Partitioning

Start with the whole data set (a single partition).

- For each partition, try every possible splitting point for each variable. Keep track of the split that decreases MSE the most.
- Split the partition containing the best split, and repeat.

Keep partitioning until some stopping criterion is reached. See `?rpart.control` for some example default settings.

>- Note: Splitting works similarly for categorical predictors.

## {data-background-image="images/dt1.png" data-background-size="contain"}

## {data-background-image="images/dt2.png" data-background-size="contain"}

## {data-background-image="images/dt3.png" data-background-size="contain"}

## Tree Pruning

>- Construct an unpruned tree $T_0$.
>- Choose a grid for $\alpha \geq 0$.
>- For each $\alpha$, find the subtree $T_\alpha$ that minimizes $\sum (y_i - \hat{y}_i)^2 + \alpha |T|$.
>    - *Fact:* $\alpha_1 < \alpha_2$ implies $T_{\alpha_1} \supseteq T_{\alpha_2}$.
>    - So you get a sequence $T_0 \supseteq T_{\alpha_1} \supseteq T_{\alpha_2} \supseteq T_{\alpha_3} \cdots$.
>- Use cross-validation to choose the best value of $\alpha$.

## {data-background-image="images/dt4.png" data-background-size="contain"}

## {data-background-image="images/dt5.png" data-background-size="contain"}

## Classification Trees

The tree growing procedure is the same for classification problems, except:

- The prediction for each leaf is the majority class in each leaf.
    - Note: Easy to handle multiple levels of $Y$.
    - Note: Easy to report predicted probabilities for each level.
- Error rate can be used instead of $\text{MSE}$.

## Measures of node purity

Let $\hat{p}_{mk}$ be the proportion of training observations in the $m$th leaf that are from the $k$th class.

>- **Gini Index:** $\displaystyle{G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})}$
>    - Smaller is better: small $G$ indicates the $\hat{p}_{mk}$s are close to zero or 1. (*purity*)
>- **Entropy:** $\displaystyle{D = -\sum_{k=1}^K \hat{p}_{mk}\log \hat{p}_{mk}}$
>    - Smaller is better: $\hat{p}_{mk}$s are close to zero or 1.
>- To evaluate the best split, $G$ and $D$ are better criteria than error rate, because they are more sensitive to node purity.
>- CV error rate can be used during pruning.

## {data-background-image="images/dt6.png" data-background-size="contain"}

## {data-background-image="images/dt7.png" data-background-size="contain"}

## Advantages/Disadvantages of Trees

- Easy to explain and interpret.
- Nice for mixture of categorical and numeric data.
- Flexible, but high variance. (Not robust.)
- Not great testing RMSE.

. . .

Next time: **Ensemble methods**

## {data-background-image="images/dt8.png" data-background-size="contain"}



# Lab: Decision trees using `rpart`
