---
title: "Learning: General Concepts"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    incremental: false
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
```


# Overview

- Learning in general
- Parameters
- Gradient Descent
- Hyperparameters and Tuning
- Tuning an xgboost model

For further reading, see [JWHT], pp. 434-435.

# Learning in general

## Supervised Learning

- Predict $Y$ from $X_1, X_2, \ldots, X_p$.
- Data: $n$ rows of the form $(y_i, x_{i1}, x_{i2}, \ldots, x_{ip})$, for $i = 1, 2, \ldots,n$.
- Use the data to train a model fit: $\hat{f}$. 

. . . 

The function $\hat{f}$ can take many different forms, e.g., a linear function, a polynomial function, a decision tree, a collection of decision trees, etc.

## Parameters

The **parameters** of a model are the collection of constants $\theta$ necessary to express $\hat{f}$ (as a formula or algorithm). For example,

>- Linear regression: $\hat{f}(x) = \hat\beta_1x_1 + \hat\beta_2x_2 + \cdots + \hat\beta_px_p$. Parameters are the model coefficients: $\theta = (\beta_1, \beta_2, \ldots, \beta_p)$.
>- Decision tree: parameters are the split points (basically).
>- Ensembles (random forests and boosted trees): parameters are the split points of all the trees.
>    - The number of parameters $N$ can be very large.
>    - $N$ is the dimension of the **parameter space.**

. . .

In general, the process of **learning** the function $\hat{f}$ amounts to determining a value $\hat\theta$ of $\theta$ that minimizes some **objective function** (e.g., RSS).

## The Gradient 

>- For a fixed set of data, the objective function $R(\theta)$ can be regarded as a function of the parameters $\theta$. 
>- Finding the best fit $\hat{f}$ amounts to finding the value of $\theta$ that minimizes $R(\theta)$.
>- Assuming that $R$ is (approximately) differentiable, the **gradient** $\nabla R$ is the vector of partial derivatives $\left(\frac{\partial R}{\partial\theta_1}, \frac{\partial R}{\partial\theta_2}, \ldots, \frac{\partial R}{\partial\theta_N}\right)$.
>    - *Fact:* At any given point $\theta$ in parameter space, $\nabla R(\theta)$ points in the direction of greatest increase in $R$.
>    - Example: simple linear regression

## Gradient Descent Algorithm

- Start with an initial guess $\theta^0$ and set $t \leftarrow 0$.
- Repeat until the objective $R$ fails to decrease:
    - $\theta^{t+1} \leftarrow \theta^t - \rho \nabla R(\theta^t)$
    - $t \leftarrow t+1$

Here $\rho > 0$ is the **learning rate**.

- Smaller values of $\rho$ require more iterations.
- Smaller values of $\rho$ will generally find more accurate minima.

## {data-background-image="images/gdesc.png" data-background-size="contain"}

## Hyperparameters and Tuning

# Lab: Tuning an `xgboost` model
