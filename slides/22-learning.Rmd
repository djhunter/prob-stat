---
title: "Learning: General Considerations"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    incremental: false
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
```


# Overview

- Learning in general
- Parameters
- Gradient Descent
- Hyperparameters and Tuning
- Tuning an xgboost model

For further reading, see [JWHT], pp. 434-435.

# Learning in general

## Supervised Learning

- Predict $Y$ from $X_1, X_2, \ldots, X_p$.
- Data: $n$ rows of the form $(y_i, x_{i1}, x_{i2}, \ldots, x_{ip})$, for $i = 1, 2, \ldots,n$.
- Use the data to train a model fit: $\hat{f}$. 

. . . 

The function $\hat{f}$ can take many different forms, e.g., a linear function, a polynomial function, a decision tree, a collection of decision trees, etc.

## Parameters

The **parameters** of a model are the collection of constants $\theta$ necessary to express $\hat{f}$ (as a formula or algorithm). For example,

>- Linear regression: $\hat{f}(x) = \hat\beta_1x_1 + \hat\beta_2x_2 + \cdots + \hat\beta_px_p$. Parameters are the model coefficients: $\theta = (\beta_1, \beta_2, \ldots, \beta_p)$.
>- Decision tree: parameters are the split points (basically).
>- Ensembles (random forests and boosted trees): parameters are the split points of all the trees.
>    - The number of parameters $N$ can be very large.
>    - $N$ is the dimension of the **parameter space.**

. . .

In general, the process of **learning** the function $\hat{f}$ amounts to determining a value $\hat\theta$ of $\theta$ that minimizes some **objective function** (e.g., RSS).

## The Gradient 

>- For a fixed set of data, the objective function $R(\theta)$ can be regarded as a function of the parameters $\theta$. 
>- Finding the best fit $\hat{f}$ amounts to finding the value of $\theta$ that minimizes $R(\theta)$.
>- Assuming that $R$ is (approximately) differentiable, the **gradient** $\nabla R$ is the vector of partial derivatives $\left(\frac{\partial R}{\partial\theta_1}, \frac{\partial R}{\partial\theta_2}, \ldots, \frac{\partial R}{\partial\theta_N}\right)$.
>    - *Fact:* At any given point $\theta$ in parameter space, $\nabla R(\theta)$ points in the direction of greatest increase in $R$.
>    - Example: simple linear regression

## Gradient Descent Algorithm

- Start with an initial guess $\theta^0$ and set $t \leftarrow 0$.
- Repeat until the objective $R$ fails to decrease:
    - $\theta^{t+1} \leftarrow \theta^t - \rho \nabla R(\theta^t)$
    - $t \leftarrow t+1$

Here $\rho > 0$ is the **learning rate**.

- Smaller values of $\rho$ require more iterations.
- Smaller values of $\rho$ will generally find more accurate minima.

## {data-background-image="images/gdesc.png" data-background-size="contain"}

## Hyperparameters and Tuning

- A numerical setting that controls the behavior of the training process is called a **hyperparameter**.
- Unlike model parameters, hyperparameters are *not directly estimated from the data*.
-    However, adjusting hyperparameters can improve the fit of a model.
-    Standard procedure: Assess hyperparameter tuning using CV, then test the tuned model on a held-out testing set (that was not used in the tuning process).

## Some XGBoost Hyperparameters

For a complete list, see the [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html).

- `eta` (learning rate). Not really something you tune. Generally, smaller is better, given reasonable time considerations.
- `gamma` (minimum loss required to make a split on a tree). Larger $\Rightarrow$ less flexible.
- `max_depth` (maximum depth of a tree). Larger $\Rightarrow$ more flexible.
- `max_leaves` (maximum number of leaves). Larger $\Rightarrow$ more flexible.
- `subsample` (percentage of rows to randomly sample for each iteration). Must be $\leq 1$, usually greater than 0.5. Can prevent overfitting.
- `colsample_bytree` (percentage of columns to randomly sample). Can deal with correlated or noisy predictors.

---

A warning from the [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/parameter.html):

> Parameter tuning is a dark art in machine learning, the optimal parameters of a model can depend on many scenarios. So it is impossible to create a comprehensive guide for doing so. 


# Lab: Tuning an `xgboost` model

